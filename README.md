<div align="center">

# ğŸ† CodeContests-O

### Feedback-Driven Iterative Test Case Generation Framework

[![arXiv](https://img.shields.io/badge/arXiv-2501.xxxxx-b31b1b.svg)](https://arxiv.org/abs/2501.xxxxx)
[![Hugging Face Dataset](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Dataset-yellow)](https://huggingface.co/datasets/caijanfeng/CodeContests-O)
[![GitHub](https://img.shields.io/badge/GitHub-Code-blue?logo=github)](https://github.com/cai-jianfeng/CodeContests-O)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)

[ğŸ“„ Paper](https://arxiv.org/abs/2501.xxxxx) | [ğŸ¤— Dataset](https://huggingface.co/datasets/caijanfeng/CodeContests-O) | [ğŸ’» Code](https://github.com/cai-jianfeng/CodeContests-O)

</div>

---

## ğŸ“¢ News

<!-- - **[2026.xx]** ğŸ‰ Paper accepted to ACL 2026! -->
- **[2026.01]** ğŸš€ Code and dataset released!

## ğŸ“– Overview

The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, **high-quality test cases for verification remain scarce**.

**CodeContests-O** addresses this challenge with a novel **Feedback-Driven Iterative Framework**. Unlike existing approaches that rely solely on LLM's intrinsic generation capabilities, our method:

1. ğŸ”„ Leverages execution feedback from both correct and incorrect solutions
2. ğŸ¯ Iteratively refines test cases toward high fidelity and discriminability
3. âœ¨ Supports both refining existing generators and creating new ones from scratch
4. ğŸ“ˆ Achieves significant improvements in test case quality

<div align="center">
<img src="assets/framework.png" width="85%">
<br>
<em>Overview of the Feedback-Driven Iterative Framework</em>
</div>

### âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Feedback-Driven** | Utilizes execution results as feedback to guide LLM in refining test cases |
| ğŸ“Š **High Quality** | 89.37% TPR & 90.89% TNR on 11M+ solutions |
| ğŸš€ **Training Effective** | +9.52% improvement on LiveCodeBench after fine-tuning |
| ğŸ› ï¸ **Extensible** | Easily adaptable to other competitive programming datasets |
| ğŸ¤— **HuggingFace Ready** | Direct integration with HuggingFace Datasets |
| âœ¨ **Generator Flexible** | Works with existing generators or creates new ones from scratch via LLM |


### ğŸ“Š Performance Comparison

<div align="center">

| Dataset | TPR (â†‘) | TNR (â†‘) | Avg (â†‘) |
|:-------:|:-------:|:-------:|:-------:|
| CodeContests | 85.05% | 81.52% | 83.29% |
| CodeContests+ | 79.00% | 83.04% | 81.02% |
| **CodeContests-O (Ours)** | **89.37%** | **90.89%** | **90.13%** |

*Evaluated on 11M+ solutions from the complete solution pool*

</div>

## ğŸš€ Quick Start

### Installation
```bash
# Clone the repository
git clone https://github.com/cai-jianfeng/CodeContests-O.git
cd CodeContests-O

# Install dependencies
pip install -e .
```

<details>
<summary>ğŸ“¦ Requirements</summary>

- Python â‰¥ 3.8
- openai â‰¥ 1.0.0
- pydantic â‰¥ 2.0.0
- requests â‰¥ 2.28.0
- tqdm â‰¥ 4.64.0
- datasets â‰¥ 2.0.0

</details>

### Prerequisites

Before running the framework, you need to download `testlib.h` - a widely-used library for competitive programming test generation:
```bash
# Download testlib.h to your working directory
wget https://raw.githubusercontent.com/MikeMirzayanov/testlib/master/testlib.h

# Or using curl
curl -O https://raw.githubusercontent.com/MikeMirzayanov/testlib/master/testlib.h
```

> ğŸ’¡ **Note**: `testlib.h` is required for compiling and running test case generators. Make sure it's accessible via the `--testlib_path` argument (defaults to `./testlib.h` in current directory).

For more information about testlib, visit the [official repository](https://github.com/MikeMirzayanov/testlib).

### Basic Usage

#### Option 1: From HuggingFace ğŸ¤—
```bash
python -m codecontests_o.main \
    --data_path ByteDance-Seed/Code-Contests-Plus \
    --results_dir ./results \
    --api_key $OPENAI_API_KEY \
    --sandbox_hosts localhost \
    --testlib_path ./testlib.h
```

#### Option 2: From Local JSON Files
```bash
python -m codecontests_o.main \
    --data_path /path/to/codecontests/json \
    --results_dir ./results \
    --api_key $OPENAI_API_KEY \
    --sandbox_hosts localhost \
    --testlib_path ./testlib.h
```

#### Option 3: Using Preset Configurations
```bash
# Development (low parallelism, debug mode)
python -m codecontests_o.main --preset development --data_path ./data

# Production (high parallelism)
python -m codecontests_o.main --preset production --data_path ./data

# Quick test (generate only, skip validation)
python -m codecontests_o.main --preset quick --data_path ./data
```

### Python API
```python
from codecontests_o import Config, CodeContestsReader, ParallelProcessor, get_preset_config
import base64

# 1. Setup configuration
config = Config.from_dict(get_preset_config("development"))
config.openai.api_key = "your-api-key"
config.dataset.data_path = "ByteDance-Seed/Code-Contests-Plus"
config.dataset.results_dir = "./results"

# 2. Load testlib.h
with open("testlib.h", "rb") as f:
    testlib_files = {"testlib.h": base64.b64encode(f.read()).decode()}

# 3. Create dataset reader (auto-detects HuggingFace vs local)
dataset = CodeContestsReader(data_path=config.dataset.data_path, split="test", start=0, end=10)

# 4. Run generation
processor = ParallelProcessor(config=config, testlib_files=testlib_files)
stats = processor.process_dataset(dataset, config.dataset.results_dir)

print(f"âœ… Completed: {stats['completed']}/{stats['total']}")
```

## ğŸ“ˆ Solution Evaluation

Evaluate solution performance on existing test cases to analyze dataset quality:
```bash
# Run evaluation
python -m codecontests_o.solutions_eval \
    --data_path ByteDance-Seed/Code-Contests-Plus \
    --subset 1x \
    --results_dir ./results_eval \
    --start 0 --end 100

# Analyze results (TPR/TNR)
python -m codecontests_o.analyze_results --results_dir ./results_eval
```

<details>
<summary>ğŸ“Š Metrics Explanation</summary>

| Metric | Description |
|--------|-------------|
| **TPR** (True Positive Rate) | Proportion of correct solutions identified as correct (â†‘ better) |
| **TNR** (True Negative Rate) | Proportion of incorrect solutions identified as incorrect (â†‘ better) |

</details>

## ğŸ”§ Custom Dataset Integration

Easily integrate your own datasets by implementing the `DatasetReader` interface:
```python
from codecontests_o.data import DatasetReader, Sample, Solution, TestCase, Language

class MyDatasetReader(DatasetReader):
    def __init__(self, data_path: str, start: int = 0, end: int = -1):
        self.data_path = data_path
        self._samples = self._load_data()
    
    def _load_data(self):
        samples = []
        for item in your_data:
            sample = Sample(
                id=item['id'],
                name=item['name'],
                description=item['description'],
                # Optional: C++ generator using testlib.h
                # - If provided: framework iteratively refines it based on feedback
                # - If None: framework generates a new generator from scratch using LLM
                generator=item.get('generator_code'),
                canonical_solutions=[Solution(code=item['solution'], language=Language.PYTHON)],
                correct_solutions=[Solution(code=item['solution'], language=Language.PYTHON)],
                incorrect_solutions=[Solution(code=item['wrong_solution'], language=Language.PYTHON)],
            )
            samples.append(sample)
        return samples
    
    def __iter__(self):
        yield from self._samples
    
    def __len__(self):
        return len(self._samples)
    
    @property
    def name(self):
        return "MyDataset"
```
```bash
python -m codecontests_o.main --custom_reader my_reader.py --data_path /path/to/data
```

## âš™ï¸ Configuration

<details>
<summary>ğŸ¤– OpenAI Configuration</summary>

| Option | Default | Description |
|--------|---------|-------------|
| `api_base` | `https://api.openai.com/v1` | API base URL |
| `api_key` | - | API key |
| `model` | `gpt-4o` | Model name |
| `max_tokens` | `8000` | Maximum tokens |

</details>

<details>
<summary>ğŸ–¥ï¸ Sandbox Configuration</summary>

| Option | Default | Description |
|--------|---------|-------------|
| `hosts` | `["localhost"]` | Sandbox hosts |
| `base_port` | `8080` | Base port |
| `port_range` | `4` | Ports per host |
| `compile_timeout` | `20` | Compilation timeout (s) |
| `run_timeout` | `20` | Execution timeout (s) |

</details>

<details>
<summary>âš¡ Processing Configuration</summary>

| Option | Default | Description |
|--------|---------|-------------|
| `max_iterations` | `3` | Max iterations per sample |
| `sample_level_workers` | `4` | Sample-level parallelism |
| `output_generation_workers` | `4` | Output generation parallelism |
| `solution_validation_workers` | `4` | Validation parallelism |

</details>

## ğŸ“ Project Structure

<details>
<summary>Click to expand</summary>

```
codecontests_o/
â”œâ”€â”€ pyproject.toml              # Build configuration
â”œâ”€â”€ setup.py                    # Installation script
â”œâ”€â”€ src/codecontests_o/         # Source code
â”‚   â”œâ”€â”€ main.py                 # Main entry point
â”‚   â”œâ”€â”€ solutions_eval.py       # Solution evaluation
â”‚   â”œâ”€â”€ analyze_results.py      # Result analysis
â”‚   â”œâ”€â”€ config/                 # Configuration management
â”‚   â”œâ”€â”€ data/                   # Data processing & readers
â”‚   â”œâ”€â”€ clients/                # OpenAI & Sandbox clients
â”‚   â”œâ”€â”€ core/                   # Generator & Validator
â”‚   â”œâ”€â”€ parallel/               # Parallel processing
â”‚   â”œâ”€â”€ prompts/                # LLM prompt templates
â”‚   â””â”€â”€ utils/                  # Utilities & logging
â””â”€â”€ README.md
```

</details>

## ğŸ“ Citation

If you find this work useful, please cite our paper:
```bibtex
@article{cai2025codecontestso,
  title={CodeContests-O: A Feedback-Driven Iterative Framework for Test Case Generation},
  author={Cai, Jianfeng and others},
  journal={arXiv preprint arXiv:2501.xxxxx},
  year={2025}
}
```

## ğŸ™ Acknowledgements

- [CodeContests](https://arxiv.org/abs/2203.07814) by DeepMind and [CodeContests+](https://arxiv.org/abs/2506.05817) by ByteDance.
- [testlib.h](https://github.com/MikeMirzayanov/testlib) by Mike Mirzayanov

## ğŸ“¬ Contact

For questions or issues, please:
- ğŸ“§ Open an [Issue](https://github.com/cai-jianfeng/CodeContests-O/issues)
- â­ Star this repo if you find it helpful!

---

<div align="center">

**[â¬† Back to Top](#-codecontests-o)**

</div>